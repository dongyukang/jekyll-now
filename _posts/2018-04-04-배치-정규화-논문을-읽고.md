---
layout: post
comments: true
title: Batch Normalization의 활용성 (공부한 자료 정리)
tags: research-paper batch-norm
---

## 인공신경망의 문제점
보통 인공 신경망 이라고하면 대게 입력층-은닉층-출력층으로 나눠져있는 fully connected layer를 많이 떠올린다. 이렇게 구조화된 신경망은 연산을 할때 내적행렬곱을 통해 weighted sum을 계산후 비선형 함수를 적용시켜 아웃풋값들을 출력 하게된다.  이 계산과정을 각 레이어마다 거친다하여 순전파 (feedforward)라고 하는데, 이 과정시 hyperparameter(한국말로는 초모수)들이 출력층에서 지대한 영향을 미치게된다. 순전파 인공신경망에서 hyperparameter란 은닉층의 갯수와 각 은닉층당 뉴런들의 갯수를 뜻한다. 이런 hypermarater들을 계속해서 튜닝함과 동시에 오차 역전파를 통해서 인공신경망은 알맞는값을 내어놓는다. 인공신경망에서 하이퍼파라미터들은 여러 복잡한 계산을 가능케 하는데 이 초모수들이 너무 많으면 흔히말하는 과적합(overfitting)의 문제를 일으킬수있다.

![overfitting](https://qph.fs.quoracdn.net/main-qimg-b4112b5d856f4f0da349460aeed854d8)

과적합이란 위사진처럼 training data에 신경망이 너무 맞춰서 학습이된 상태를 말한다. 결국 training accuracy는 높을지 몰라도 validation과정시 validation data가 training data패턴에 없던 data이면 신경망은 빗나간 예측을 할것이다. 결국 generalization의 큰 문제가 생긴다. 이런 과적합문제 말고도 인공신경망은 레이어망 hyperparameter에 대한 여러가지 문제점을 지니는데, 또다른 문제점은 심층 신경망에서 순전파 계산시 은닉 뉴런들을 연결하는 가중치들중 몇몇값들이 평균 가중치값보다 클경우 계속해서 weighted sum이되면서 각 레이어의 분포가 달라진다는 것이다. 이것을 Internal Covariate Shift현상이라 논문에서는 정의한다. 말만 어렵지 결국 input distribution이 각 레이어마다 다르다는뜻. 옛날 '가족 오락관'이라는 프로그램의 '고요속의 외침'이라는 게임에서 출연진들은 외부 소리가 잘 안들리는 헤드폰을 착용하고 차례대로 뒷사람에게 제시된단어를 크게 소리치며 전달을 한다. 그런데 잘 안들리니 뒤로 갈수록 단어가 이상하게 변질되어 전달이된다. 이처럼 레이어가 뒤로갈수록 분포가 변화되어 결국 출력층에 안좋은 영향을 끼칠수 있게된다. Vanishing Gradient 혹은 Gradient Explosion등이 이러한 문제가 될수 있겠다. 이러한 현상을 방지하고자 논문에서는 각 네트워크망의 distribution을 같게하는 방법을 고안했고 이것을 저자는 Batch Normalization(배치 정규화)이라 소개한다. 논문에서 소개하는 배치정규화의 이점은 학습속도의 향상과 자체적인 regularization효과라고 주장하고있다.

[논문링크-Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate   Shift](https://arxiv.org/pdf/1502.03167.pdf)

## 배치 정규화 알고리즘

많은 리서치 연구원들이 위에서 나열했던 인공신경망의 문제들을 해결하기위해 여러방법들을 동원해왔다.  Vanishing Gradient현상을 없애기위해 Rectified linear unit(relu)활성화 함수를 고안했고, 평균보다큰 가중치값들을 줄이기위해 L-2 혹은 L-1 regularizer weight decay기법, 그리고 각 레이어마다 랜덤으로 뉴런을 꺼버려 노이즈를 발생시켜 가중치의 편향을 방지해주는 Dropout기법등, 다양한 방법들로 인공신경망의 문제점들을 개선하려 여러노력을 해왔다. 하지만 이런 방법들은 대게 데이터에다 직접 손을대지않는 간접적으로 해결을 하는 방식이라 할수있겠다. 하지만 배치 정규화는 네트워크망 내부 데이터를 아예 안정적으로 학습을 할수있도록 만들어 버린다. 논문에서는 데이터가 whitened, linearly transformed to zero mean, unit variance, decorrelated(무상관화)가 되었을때 converge를 빨리한다고 주장한다. 결국 평균0 표준편차 1인 분포로 만들면 된다는 것이다. (좀더 디테일하게 들어가면 이렇게만 하면 안돼고 다른 변수들이 등장하긴한다.) 아래는 논문에있는 normalize 과정이다.

![normalize](https://shuuki4.files.wordpress.com/2016/01/bn1.png)

과정을 살펴보면 미니배치의 평균과 각 데이터와의 거리를 제곱한것의 합의 평균, 즉, 분산(Variance)을 구한다. 그리고 각 데이터와 미니배치의 평균의 거리를 분산에 numerical stability를위해 epsilon이라는 미세한값을 더한것의 제곱근으로 나누어준다. 결국 표준편차로 나눈다. 그냥 Z-Score다 <s>시발</s>.  하지만 여기서 끝이아니라 gamma값을 곱하고 beta값을 더해줌으로써 scale과 shift를 한다. 이것은 gamma와 beta를 trainable하게하는 파라미터로써 손실함수를 각 파리미터에 대해서 편미분을 하고 경사하강법으로 gamma값과 beta값을 업데이트를 할수있음을 뜻한다. 결국 이말은 batch normalization하는 과정은 어떻게보면 extra layer로 볼수있겠다. 아래사진처럼 어파인 layer의 값들을 활성화함수로 넘겨주기전에 정규화를 한후 비선형 활성화 함수를 적용하는 것이다.

![batchnorm](https://i0.wp.com/mohammadpz.github.io/posts/2015_02_01_IFT6266_Cats_vs_Dogs/img/bn.png)

## 배치 정규화의 이점
Batch Normalization의 가장 궁극적인 목표는 internal covariate shift를 줄이는 것이다. 신경망의 각 레이어의 분포를 같게함으로써 좀더 안정적인 학습이 가능하다. 하지만 더 나아가 논문에서는 배치 정규화를 사용함으로써 추가로 2가지 이점이 더 있다 설명한다.

### 첫번째. 빠른 학습속도
보통 경사하강을 통해 학습을 하려고보면 학습률(learning rate)을 고민할때가 많다. 안전빵으로 굉장히 작게잡으면 되겠거니 했는데 학습시간이 굉장히 오래걸린다. 특히 내 컴퓨터로 학습을 하면 비행기 이륙하는듯한 컴퓨터 팬소리를 하루종일 들어야한다. 그렇다고 높게잡으면 gradient vanishing현상이나 explosion현상때문에 골머리를 앓는다. 하지만 배치 정규화를 넣으면 parameter들의 scale에 학습률이 크게 작용을 하지않기때문에 높은 학습률 사용이 가능하다는 것이다. 그래서 학습이 비교적 빠르다고 할수있다.

### 두번째. 자체적인 regularization
논문은 배치 정규화를 사용함으로써 자체적인 regularization효과가 있다고 주장한다. 따라서 weight decay나 dropout등을 굳이 쓰지않아도 된다고 주장한다.

## 직접적인 실험
배치정규화의 실용성과 효과를 실험해 보기위해 Kaggle에서 제공하는 [Titanic 데이터셋](https://www.kaggle.com/c/titanic/data)을 이용하여 Pclass, Fare, Age들을 입력데이터, 그리고 Survived를 아웃풋으로 하여 이 두 인풋과 아웃풋의 관계를 학습시켜 보았다. 인공신경망 type은 그냥 일반 Feedforward Neural Network를 사용했으며 앞전에 언급했다시피, 입력층은 3개의 뉴런으로 구성된 Pclass, Fare, Age등이고 출력층은 Survived인, 이중분류이다.  신경망의 architecture은 2개의 은닉층이있는 심층 인공신경망으로 구성했다. 전체적인 코드는 귀찮으니 포스팅은 하지 않겠지만 배치 정규화를 적용한 버전과 그렇지않은 버전의 accuracy는 차이가났다.  학습률, 배치사이즈등 모든 조건은 같은 상태에서 학습을 진행했다. BN을 적용하지않은 신경망의경우 epoch을 200번 돌렸을때의 정확도는 69%에 머물렀고
BN을 적용한 신경망의경우 같은 epoch후 71%까지 오르는것을 확인할수 있었다.

아래는 배치 정규화를 적용하지 않은 결과이다.

![nobatch_loss](https://i.imgur.com/C1VqJ56.png)
![nobatch_acc](https://i.imgur.com/KYXwtjw.png)

배치 정규화를 적용한 신경망의 경우

![batch_loss](https://i.imgur.com/w3jtoKk.png)
![batch acc](https://i.imgur.com/hmKl9iU.png)

언뜻봐도 배치 정규화를 적용한 신경망의 validation accuracy나 validation loss가 training dataset과 확실히 차이가 나는것을 볼수가있다. 물론 배치 정규화를 사용하지 않은 신경망에서 정확도가 75%까지 올라가긴 했지만 고대로 69%로 내려왔다. 안정적이지 못하다. 이것에 반해 배치정규화를 적용한 신경망의 경우 비교적 안정적으로 정확도가 올라가는것을 확인할수 있다. 만약 epoch수가 더 많았다면 아마 결과는 더 좋았을지도 모르겠다.

## 마치며
이로인해 나는 드롭아웃이나 regularizer등의 간접적인 방법도 좋지만, 아예 신경망 내부데이터를 직접적으로 건드려버리는 이런 신개념적인 접근방법도 참신하다고 생각한다. 그리고 하나더, 정규화한후의 값들을 scale과 shift과정을 거칠때 gamma 값을 표준편차로두고 beta값을 평균값으로 두면 다시 정규화 하기전 값으로 돌아간다. 나는 이런 면들이 배치 정규화로 하여금 굉장히 powerful한 방법으로 여기게끔 만든다. 이번 논문을 읽으며, 완전히 다 이해하지는 못했지만, 그래도 배워가는게 많다고 생각한다. 머신러닝에 있어서 알고리즘도 중요하지만 데이터의 중요성을 다시한번 깨닫는 경험이었다. 아무리 알고리즘이 훌륭하고 최고의 computational resource가 있다하더라도 데이터가 쓰레기면 그냥 알고리즘에 noise를 fitting하는거 밖에 안될것이다.


{% include disqus.html %}
